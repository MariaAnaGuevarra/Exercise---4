{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00dacc7b",
   "metadata": {},
   "source": [
    "### Exercise 4 - Maria Ana P. Guevarra\n",
    "\n",
    "### Part1: Load the tagged sentences of the CONLL2000 test set (test.txt). Make sure that only the NP tags are labelled.\n",
    "\n",
    "1.\tExamine the first 20 entries\n",
    "2.\tCreate chunking/chinking rules that classifies 100% of the NP tags in the first 20 entries.\n",
    "3.\tApply the rules on the other entries\n",
    "4.\tCalculate the accuracy, precision, recall, and F-measure of IOB tags\n",
    "5.\tCalculate the confusion matrix on IOB tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ceb84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c206907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f37ee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('White', 'NNP'), ('Males', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('White', 'NNP'), ('Females', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('Black', 'NNP'), ('Males', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('Black', 'NNP'), ('Females', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('Skoal', 'NNP'), ('Daze', 'NNP')])]),\n",
       " Tree('S', [Tree('NP', [('Daffynition', 'NN')])]),\n",
       " Tree('S', [Tree('NP', [('Citicorp', 'NNP')])]),\n",
       " Tree('S', [Tree('NP', [('Wells', 'NNP'), ('Fargo', 'NNP')])]),\n",
       " Tree('S', [Tree('NP', [('Manufacturers', 'NNP'), ('Hanover', 'NNP')])]),\n",
       " Tree('S', [Tree('NP', [('PNC', 'NNP'), ('Financial', 'NNP')])]),\n",
       " Tree('S', [Tree('NP', [('Treasury', 'NNP'), ('Securities', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('Corporate', 'JJ'), (',', ','), ('Other', 'JJ'), ('Issues', 'NNS')])]),\n",
       " Tree('S', [Tree('NP', [('Mortgage-Backed', 'JJ'), ('Securities', 'NNPS')])]),\n",
       " Tree('S', [Tree('NP', [('Municipals', 'NNS')])]),\n",
       " Tree('S', [Tree('NP', [('Foreign', 'JJ'), ('Bonds', 'NNS')])]),\n",
       " Tree('S', [Tree('NP', [('SHEARSON', 'NNP'), ('LEHMAN', 'NNP'), ('HUTTON', 'NNP'), ('Inc', 'NNP')]), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('Merck', 'NNP'), ('&', 'CC'), ('Co', 'NNP')]), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('Warner-Lambert', 'NNP'), ('Co', 'NNP')]), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('Eli', 'NNP'), ('Lilly', 'NNP'), ('&', 'CC'), ('Co', 'NNP')]), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('BRIEFS', 'NNS')]), (':', ':')])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_trees = sorted(nltk.corpus.conll2000.chunked_sents('test.txt'), key=len)\n",
    "conll_trees[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d395c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledata = conll_trees[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806913cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Courts', 'NNS', 'B-NP'),\n",
       " ('and', 'CC', 'I-NP'),\n",
       " ('legislatures', 'NNS', 'I-NP'),\n",
       " ('make', 'VBP', 'B-VP'),\n",
       " ('decisions', 'NNS', 'B-NP'),\n",
       " ('in', 'IN', 'B-PP'),\n",
       " ('very', 'RB', 'B-NP'),\n",
       " ('different', 'JJ', 'I-NP'),\n",
       " ('ways', 'NNS', 'I-NP'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('are', 'VBP', 'B-VP'),\n",
       " ('each', 'DT', 'O'),\n",
       " ('susceptible', 'JJ', 'O'),\n",
       " ('to', 'TO', 'B-PP'),\n",
       " ('very', 'RB', 'B-NP'),\n",
       " ('different', 'JJ', 'I-NP'),\n",
       " ('kinds', 'NNS', 'I-NP'),\n",
       " ('of', 'IN', 'B-PP'),\n",
       " ('errors', 'NNS', 'B-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_iob = nltk.chunk.tree2conlltags(sampledata)\n",
    "sample_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076a955e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Courts', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('legislatures', 'NNS'),\n",
       " ('make', 'VBP'),\n",
       " ('decisions', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('very', 'RB'),\n",
       " ('different', 'JJ'),\n",
       " ('ways', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('are', 'VBP'),\n",
       " ('each', 'DT'),\n",
       " ('susceptible', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('very', 'RB'),\n",
       " ('different', 'JJ'),\n",
       " ('kinds', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('errors', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_pos = [(w, pos) for w, pos, iob in sample_iob]\n",
    "sample_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a5b0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 403)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2000_data = nltk.corpus.conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "conll2000_train, conll2000_test = train_test_split(conll2000_data, test_size=0.2, random_state=0)\n",
    "len(conll2000_train), len(conll2000_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbcf63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Courts/NNS)\n",
      "  and/CC\n",
      "  (NP legislatures/NNS)\n",
      "  make/VBP\n",
      "  (NP decisions/NNS)\n",
      "  in/IN\n",
      "  very/RB\n",
      "  (NP different/JJ ways/NNS)\n",
      "  and/CC\n",
      "  are/VBP\n",
      "  each/DT\n",
      "  susceptible/JJ\n",
      "  to/TO\n",
      "  very/RB\n",
      "  (NP different/JJ kinds/NNS)\n",
      "  of/IN\n",
      "  (NP errors/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<NNP><NNP><NNP><NNP><.>}\n",
    "        {<NNP><NNP><CC><NNP><.>}\n",
    "        {<NNP><CC><NNP><.>}\n",
    "        {<JJ><,><JJ><NNS>}\n",
    "        {<NNP><NNP><.>}\n",
    "        {<NNP><NNP>}\n",
    "        {<NNP><NNPS>}\n",
    "        {<JJ><NNPS>}\n",
    "        {<JJ><NNS>}\n",
    "        {<NNP>}\n",
    "        {<NNS>}\n",
    "        {<NN>}\n",
    "        }<VBZ|RB|VBG|IN>{ \n",
    "\"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "print(chunker.parse(sample_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c860054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  30.5%%\n",
      "    Precision:     23.5%%\n",
      "    Recall:        14.0%%\n",
      "    F-Measure:     17.5%%\n"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(conll_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b39206",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Part2: Load the tagged sentences of the CONLL2000 with the NP tags. Use the train data (train.txt) as the training set and the test data (test.txt) as the test set.\n",
    "1.\tCreate a Bigram chunker.\n",
    "2.\tCreate a Trigram chunker with a Bigram chunker backoff. \n",
    "3.\tCompare the Bigram and the Trigram chunker with backoff on accuracy, precision, recall, and F-measure of IOB tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7fe819",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_train = nltk.corpus.conll2000.chunked_sents('train.txt',chunk_types=('NP'))\n",
    "conll_test = nltk.corpus.conll2000.chunked_sents('test.txt', chunk_types=('NP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dec34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68667cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = BigramChunker(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d8dd2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "print(bigram_chunker.evaluate(conll_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4728ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramBackoffChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        backofftagger = nltk.BigramTagger(train_data)\n",
    "        self.tagger = nltk.TrigramTagger(train_data,backoff=backofftagger) \n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7c53c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_backoff_chunker = TrigramBackoffChunker(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9073d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.4%%\n",
      "    Precision:     82.5%%\n",
      "    Recall:        86.9%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "print(trigram_backoff_chunker.evaluate(conll_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10a4fb5",
   "metadata": {},
   "source": [
    "##### The ChunkParse score of the two seems to be even. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98d839",
   "metadata": {},
   "source": [
    "\n",
    "### Part3: Load the dutch NER tagged sentences of the CONLL2002 with the NP tags. Use the train data (ned.train) as the training set and the test data A (ned.testa) as the test set.\n",
    "1.\tCreate a CRF chunker\n",
    "2.\tCreate 4 tag features and 8 word based features\n",
    "3.\tCalculate the confusion matrix. The diagonals of the confusion matrix should have the largest or 2nd largest number on its row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9263b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ngram_fmt(data, label=True):\n",
    "    if label:\n",
    "        return [\n",
    "            [(pos, iob) for w, pos, iob in nltk.chunk.tree2conlltags(s)]\n",
    "            for s in data\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            [pos for w, pos, iob in nltk.chunk.tree2conlltags(s)]\n",
    "            for s in data\n",
    "        ]\n",
    "\n",
    "def to_evaluate(data):\n",
    "    return [\n",
    "        w[-1]\n",
    "        for s in data\n",
    "        for w in (nltk.chunk.tree2conlltags(s) if isinstance(s, nltk.tree.Tree) else s)\n",
    "    ]\n",
    "\n",
    "\n",
    "def to_crf_fmt(data, label=True):\n",
    "    if label:\n",
    "        return [\n",
    "            [((w, pos), iob) for w, pos, iob in nltk.chunk.tree2conlltags(s)]\n",
    "            for s in data\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            [(w, pos) for w, pos, phr in nltk.chunk.tree2conlltags(s)]\n",
    "            for s in data\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7de459e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15806, 2895)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2002train = nltk.corpus.conll2002.chunked_sents('ned.train')\n",
    "conll2002test = nltk.corpus.conll2002.chunked_sents('ned.testa')\n",
    "len(conll2002train), len(conll2002test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fe09915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag_word_features(tokens, idx):\n",
    "    word = tokens[idx][0]\n",
    "    postag = tokens[idx][1]\n",
    "   \n",
    "    feature_list = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag\n",
    "    ]\n",
    "\n",
    "\n",
    "    if idx > 0:\n",
    "            word1 = tokens[idx-1][0]\n",
    "            postag1 = tokens[idx-1][1]\n",
    "            feature_list.extend([\n",
    "                '-1:word.lower=' + word1.lower(),\n",
    "                '-1:word.istitle=%s' % word1.istitle(),\n",
    "                '-1:word.isupper=%s' % word1.isupper(),\n",
    "                '-1:word.isdigit=%s' % word1.isdigit(),\n",
    "                '-1:postag=' + postag1\n",
    "            ])\n",
    "    else:\n",
    "          \n",
    "            feature_list.append('BOS')\n",
    "\n",
    "\n",
    "    if idx < len(tokens)-1:\n",
    "        word1 = tokens[idx+1][0]\n",
    "        postag1 = tokens[idx+1][1]\n",
    "        feature_list.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:word.isdigit=%s' % word1.isdigit(),\n",
    "            '+1:postag=' + postag1\n",
    "        ])\n",
    "    else:\n",
    "        \n",
    "        feature_list.append('EOS')\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae2d9c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.75      0.73      0.74       479\n",
      "      B-MISC       0.80      0.62      0.70       748\n",
      "       B-ORG       0.88      0.57      0.69       686\n",
      "       B-PER       0.66      0.82      0.73       703\n",
      "       I-LOC       0.55      0.42      0.48        64\n",
      "      I-MISC       0.56      0.48      0.52       215\n",
      "       I-ORG       0.85      0.60      0.70       396\n",
      "       I-PER       0.75      0.94      0.83       423\n",
      "           O       0.99      1.00      0.99     33973\n",
      "\n",
      "    accuracy                           0.97     37687\n",
      "   macro avg       0.75      0.69      0.71     37687\n",
      "weighted avg       0.97      0.97      0.97     37687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunker = nltk.crf.CRFTagger(feature_func=ner_tag_word_features)\n",
    "chunker.train(to_crf_fmt(conll2002train), 'crf_ner_tag_word.tag')\n",
    "iob_predict = chunker.tag_sents(to_crf_fmt(conll2002test, label=False))\n",
    "print(classification_report(to_evaluate(conll2002test), to_evaluate(iob_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "299c027d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  352,    12,     6,    76,     2,     7,     0,     1,    23],\n",
       "       [   38,   461,    18,    55,     1,     5,     0,     4,   166],\n",
       "       [   52,    57,   388,   122,     1,     3,     5,     2,    56],\n",
       "       [   15,    24,    16,   576,     0,     2,     4,    20,    46],\n",
       "       [    2,     1,     0,     1,    27,     5,     6,    18,     4],\n",
       "       [    3,     4,     3,     5,    10,   104,    14,    23,    49],\n",
       "       [    7,     6,     4,     8,     6,    35,   237,    54,    39],\n",
       "       [    0,     0,     0,     5,     0,    12,     7,   396,     3],\n",
       "       [    2,    11,     6,    21,     2,    14,     7,     8, 33902]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(to_evaluate(conll2002test), to_evaluate(iob_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
